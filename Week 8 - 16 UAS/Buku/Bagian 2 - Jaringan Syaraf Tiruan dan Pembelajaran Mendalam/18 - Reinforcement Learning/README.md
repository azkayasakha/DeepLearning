---

# ğŸ“˜ Chapter 18 â€“ Reinforcement Learning

---

## ğŸ¯ Tujuan Bab Ini:

* Memahami konsep dasar **Reinforcement Learning (RL)**
* Membedakan antara RL vs Supervised Learning
* Implementasi **Q-learning** dan **Deep Q-Network (DQN)**
* Menggunakan **OpenAI Gym** sebagai environment interaktif
* Membuat agen belajar melalui eksplorasi dan imbalan

---

## ğŸ” 1. Apa itu Reinforcement Learning?

> **RL** adalah pendekatan pembelajaran di mana **agen** belajar mengambil keputusan dengan **berinteraksi dengan lingkungan** untuk **memaksimalkan reward jangka panjang**.

---

## ğŸ§© 2. Komponen dalam RL

| Komponen        | Deskripsi                                     |
| --------------- | --------------------------------------------- |
| **Agent**       | Entitas yang belajar dan bertindak            |
| **Environment** | Dunia tempat agent berinteraksi               |
| **State (s)**   | Situasi saat ini dalam environment            |
| **Action (a)**  | Tindakan yang diambil oleh agent              |
| **Reward (r)**  | Angka yang menunjukkan nilai tindakan         |
| **Policy (Ï€)**  | Strategi pemilihan aksi berdasarkan state     |
| **Episode**     | Satu siklus percobaan (dari awal ke terminal) |

---

## ğŸ§  3. Goal RL

Agen belajar **policy optimal** untuk **memaksimalkan expected cumulative reward** (sering disebut sebagai **return** atau `G_t`).

---

## ğŸ“ˆ 4. Value Function dan Q-Function

* **Value function (V)**: ekspektasi reward dari state tertentu
* **Q-function (Q)**: ekspektasi reward dari state + action tertentu

---

## ğŸ² 5. Eksplorasi vs Eksploitasi

* **Eksploitasi**: pilih aksi terbaik yang diketahui
* **Eksplorasi**: coba aksi baru untuk belajar
* Strategi umum: **Îµ-greedy** (kadang eksplorasi, kadang eksploitasi)

---

## ğŸ§  6. Q-Learning (Tabular)

> Algoritma RL klasik untuk **mengisi tabel Q(s, a)**:

### Update Rule:

```
Q(s,a) â† Q(s,a) + Î± [r + Î³ * max_aâ€™ Q(sâ€™,aâ€™) - Q(s,a)]
```

* Î± = learning rate
* Î³ = discount factor
* sâ€™ = state berikutnya

---

## ğŸ§ª 7. Contoh: FrozenLake-v0 (OpenAI Gym)

Environment sederhana: Agen bergerak di grid menuju tujuan sambil menghindari lubang es.

```python
import gym
env = gym.make("FrozenLake-v1", is_slippery=False)
obs = env.reset()
```

---

## ğŸ¤– 8. Deep Q-Network (DQN)

Ketika **jumlah state/action terlalu besar**, kita tidak bisa pakai tabel â†’ pakai neural network!

```python
Q(s, a) â‰ˆ f(s, a; Î¸)
```

### Komponen DQN:

* Neural network Q(s, a)
* Replay buffer: menyimpan pengalaman
* Target network: salinan stabil untuk perhitungan Q-target

---

## ğŸ” 9. Training Loop DQN (Inti)

1. Dapatkan state `s`
2. Pilih aksi `a` dengan Îµ-greedy
3. Lakukan aksi â†’ dapat `r`, `s'`
4. Simpan (s, a, r, sâ€™) ke replay buffer
5. Sampling minibatch
6. Hitung Q-target: `r + Î³ max Q(sâ€™, aâ€™)`
7. Update Q-network

---

## ğŸ§  10. Perbedaan RL dengan Supervised Learning

| Aspek    | Supervised Learning | Reinforcement Learning  |
| -------- | ------------------- | ----------------------- |
| Label    | Ada                 | Tidak langsung (reward) |
| Tujuan   | Prediksi akurat     | Maksimalkan reward      |
| Training | Sekali (static)     | Interaktif              |
| Data     | Diberikan           | Dihasilkan sendiri      |

---

## ğŸ§  Bonus: Policy Gradient & Actorâ€“Critic

* **Policy gradient** langsung melatih `Ï€(a|s)`
* **Actorâ€“Critic**: dua model â†’ actor (policy), critic (value function)

---

## ğŸ“‘ Ringkasan Inti Bab Ini:

* RL = belajar dari interaksi dan reward
* Agen belajar **policy** untuk mencapai tujuan jangka panjang
* Q-Learning mengisi tabel Q(s,a)
* DQN menggunakan neural network untuk memperkirakan Q(s,a)
* Konsep penting: eksplorasi vs eksploitasi, reward, value, policy

---