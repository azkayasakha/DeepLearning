{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "📥 1. Load & Split Dataset"
      ],
      "metadata": {
        "id": "q5nfQrb-GuJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalisasi skala pixel: 0–255 → 0–1\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Bagi menjadi train dan validasi\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "metadata": {
        "id": "IDBHxT84GwAP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 2. Fungsi Preprocessing"
      ],
      "metadata": {
        "id": "eh1R0ITqGy6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(X, y):\n",
        "    X = tf.cast(X, tf.float32)\n",
        "    X = tf.expand_dims(X, -1)  # ubah shape jadi (28, 28, 1)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "yJmz7nGJG0JP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎨 3. (Opsional) Fungsi Augmentasi Gambar"
      ],
      "metadata": {
        "id": "GbsM04RgG2sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(X, y):\n",
        "    X = tf.image.random_flip_left_right(X)\n",
        "    X = tf.image.random_brightness(X, max_delta=0.1)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "gOWHr-9pG3y4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧵 4. Membuat Pipeline tf.data"
      ],
      "metadata": {
        "id": "YcnR5u0lG6CW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .shuffle(buffer_size=10000)\n",
        "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "iclpw-wjG9A9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
        "valid_dataset = (\n",
        "    valid_dataset\n",
        "    .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    .batch(32)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "Q7EXLMfqG_cL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🏗️ 5. Bangun dan Latih Model"
      ],
      "metadata": {
        "id": "ePYOS3bOHB96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models, layers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=[28, 28, 1]),\n",
        "    layers.Conv2D(32, 3, activation=\"relu\"),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation=\"relu\"),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(train_dataset, epochs=10, validation_data=valid_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Uah4fbCHCY_",
        "outputId": "0ae686dd-a1e3-4d86-81b8-13f9e1aca72e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7557 - loss: 0.6849 - val_accuracy: 0.8718 - val_loss: 0.3503\n",
            "Epoch 2/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.8738 - loss: 0.3420 - val_accuracy: 0.8948 - val_loss: 0.2928\n",
            "Epoch 3/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.8928 - loss: 0.2909 - val_accuracy: 0.9062 - val_loss: 0.2523\n",
            "Epoch 4/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9043 - loss: 0.2581 - val_accuracy: 0.9128 - val_loss: 0.2425\n",
            "Epoch 5/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9138 - loss: 0.2362 - val_accuracy: 0.9118 - val_loss: 0.2433\n",
            "Epoch 6/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9185 - loss: 0.2186 - val_accuracy: 0.9124 - val_loss: 0.2374\n",
            "Epoch 7/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9247 - loss: 0.2038 - val_accuracy: 0.9070 - val_loss: 0.2488\n",
            "Epoch 8/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9300 - loss: 0.1884 - val_accuracy: 0.9234 - val_loss: 0.2274\n",
            "Epoch 9/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9343 - loss: 0.1741 - val_accuracy: 0.9154 - val_loss: 0.2474\n",
            "Epoch 10/10\n",
            "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.9388 - loss: 0.1652 - val_accuracy: 0.9192 - val_loss: 0.2249\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79a957351850>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Catatan:\n",
        "\n",
        "* Kita tidak menggunakan `X_train` langsung, tetapi melalui `tf.data.Dataset`, agar efisien dan scalable.\n",
        "* `.map()` digunakan untuk menerapkan fungsi preprocessing dan augmentasi.\n",
        "* `.prefetch(tf.data.AUTOTUNE)` membantu mempercepat training dengan menjalankan pipeline dan model secara paralel.\n",
        "* `.expand_dims()` penting agar data masuk ke CNN dalam format `(28, 28, 1)`."
      ],
      "metadata": {
        "id": "OVR4CIZnHJQf"
      }
    }
  ]
}