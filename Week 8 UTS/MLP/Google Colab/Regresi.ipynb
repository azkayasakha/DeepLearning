{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDFzfTrWHTei"
      },
      "outputs": [],
      "source": [
        "!pip install keras_tuner\n",
        "!pip install scikeras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "from imblearn.combine import SMOTETomek\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks, optimizers, regularizers\n",
        "import os\n",
        "import shap\n",
        "import keras_tuner as kt\n",
        "from scikeras.wrappers import KerasRegressor, KerasClassifier\n",
        "from sklearn.ensemble import VotingRegressor, VotingClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3P_VW7qKI1fP"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXPekGRFI3cJ",
        "outputId": "75422598-2d49-447f-9ebc-5f21e4a9a4df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing data...\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "print(\"Loading and preprocessing data...\")\n",
        "df = pd.read_csv(\"RegresiUTSTelkom.csv\")\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQqyfjJeJC35",
        "outputId": "302e1d4a-c9b6-4e12-a621-adc86eded7a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (515130, 91)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 515130 entries, 0 to 515343\n",
            "Data columns (total 91 columns):\n",
            " #   Column      Non-Null Count   Dtype  \n",
            "---  ------      --------------   -----  \n",
            " 0   2001        515130 non-null  int64  \n",
            " 1   49.94357    515130 non-null  float64\n",
            " 2   21.47114    515130 non-null  float64\n",
            " 3   73.0775     515130 non-null  float64\n",
            " 4   8.74861     515130 non-null  float64\n",
            " 5   -17.40628   515130 non-null  float64\n",
            " 6   -13.09905   515130 non-null  float64\n",
            " 7   -25.01202   515130 non-null  float64\n",
            " 8   -12.23257   515130 non-null  float64\n",
            " 9   7.83089     515130 non-null  float64\n",
            " 10  -2.46783    515130 non-null  float64\n",
            " 11  3.32136     515130 non-null  float64\n",
            " 12  -2.31521    515130 non-null  float64\n",
            " 13  10.20556    515130 non-null  float64\n",
            " 14  611.10913   515130 non-null  float64\n",
            " 15  951.0896    515130 non-null  float64\n",
            " 16  698.11428   515130 non-null  float64\n",
            " 17  408.98485   515130 non-null  float64\n",
            " 18  383.70912   515130 non-null  float64\n",
            " 19  326.51512   515130 non-null  float64\n",
            " 20  238.11327   515130 non-null  float64\n",
            " 21  251.42414   515130 non-null  float64\n",
            " 22  187.17351   515130 non-null  float64\n",
            " 23  100.42652   515130 non-null  float64\n",
            " 24  179.19498   515130 non-null  float64\n",
            " 25  -8.41558    515130 non-null  float64\n",
            " 26  -317.87038  515130 non-null  float64\n",
            " 27  95.86266    515130 non-null  float64\n",
            " 28  48.10259    515130 non-null  float64\n",
            " 29  -95.66303   515130 non-null  float64\n",
            " 30  -18.06215   515130 non-null  float64\n",
            " 31  1.96984     515130 non-null  float64\n",
            " 32  34.42438    515130 non-null  float64\n",
            " 33  11.7267     515130 non-null  float64\n",
            " 34  1.3679      515130 non-null  float64\n",
            " 35  7.79444     515130 non-null  float64\n",
            " 36  -0.36994    515130 non-null  float64\n",
            " 37  -133.67852  515130 non-null  float64\n",
            " 38  -83.26165   515130 non-null  float64\n",
            " 39  -37.29765   515130 non-null  float64\n",
            " 40  73.04667    515130 non-null  float64\n",
            " 41  -37.36684   515130 non-null  float64\n",
            " 42  -3.13853    515130 non-null  float64\n",
            " 43  -24.21531   515130 non-null  float64\n",
            " 44  -13.23066   515130 non-null  float64\n",
            " 45  15.93809    515130 non-null  float64\n",
            " 46  -18.60478   515130 non-null  float64\n",
            " 47  82.15479    515130 non-null  float64\n",
            " 48  240.5798    515130 non-null  float64\n",
            " 49  -10.29407   515130 non-null  float64\n",
            " 50  31.58431    515130 non-null  float64\n",
            " 51  -25.38187   515130 non-null  float64\n",
            " 52  -3.90772    515130 non-null  float64\n",
            " 53  13.29258    515130 non-null  float64\n",
            " 54  41.5506     515130 non-null  float64\n",
            " 55  -7.26272    515130 non-null  float64\n",
            " 56  -21.00863   515130 non-null  float64\n",
            " 57  105.50848   515130 non-null  float64\n",
            " 58  64.29856    515130 non-null  float64\n",
            " 59  26.08481    515130 non-null  float64\n",
            " 60  -44.5911    515130 non-null  float64\n",
            " 61  -8.30657    515130 non-null  float64\n",
            " 62  7.93706     515130 non-null  float64\n",
            " 63  -10.7366    515130 non-null  float64\n",
            " 64  -95.44766   515130 non-null  float64\n",
            " 65  -82.03307   515130 non-null  float64\n",
            " 66  -35.59194   515130 non-null  float64\n",
            " 67  4.69525     515130 non-null  float64\n",
            " 68  70.95626    515130 non-null  float64\n",
            " 69  28.09139    515130 non-null  float64\n",
            " 70  6.02015     515130 non-null  float64\n",
            " 71  -37.13767   515130 non-null  float64\n",
            " 72  -41.1245    515130 non-null  float64\n",
            " 73  -8.40816    515130 non-null  float64\n",
            " 74  7.19877     515130 non-null  float64\n",
            " 75  -8.60176    515130 non-null  float64\n",
            " 76  -5.90857    515130 non-null  float64\n",
            " 77  -12.32437   515130 non-null  float64\n",
            " 78  14.68734    515130 non-null  float64\n",
            " 79  -54.32125   515130 non-null  float64\n",
            " 80  40.14786    515130 non-null  float64\n",
            " 81  13.0162     515130 non-null  float64\n",
            " 82  -54.40548   515130 non-null  float64\n",
            " 83  58.99367    515130 non-null  float64\n",
            " 84  15.37344    515130 non-null  float64\n",
            " 85  1.11144     515130 non-null  float64\n",
            " 86  -23.08793   515130 non-null  float64\n",
            " 87  68.40795    515130 non-null  float64\n",
            " 88  -1.82223    515130 non-null  float64\n",
            " 89  -27.46348   515130 non-null  float64\n",
            " 90  2.26327     515130 non-null  float64\n",
            "dtypes: float64(90), int64(1)\n",
            "memory usage: 361.6 MB\n",
            "\n",
            "Descriptive statistics:\n",
            "                2001       49.94357       21.47114        73.0775  \\\n",
            "count  515130.000000  515130.000000  515130.000000  515130.000000   \n",
            "mean     1998.396300      43.386243       1.284453       8.658865   \n",
            "std        10.931639       6.067918      51.583820      35.270798   \n",
            "min      1922.000000       1.749000    -337.092500    -301.005060   \n",
            "25%      1994.000000      39.953433     -26.065532     -11.463113   \n",
            "50%      2002.000000      44.257105       8.412635      10.476855   \n",
            "75%      2006.000000      47.833555      36.121255      29.766593   \n",
            "max      2011.000000      61.970140     384.065730     322.851430   \n",
            "\n",
            "             8.74861      -17.40628      -13.09905      -25.01202  \\\n",
            "count  515130.000000  515130.000000  515130.000000  515130.000000   \n",
            "mean        1.164394      -6.553821      -9.521523      -2.391044   \n",
            "std        16.322518      22.861826      12.858266      14.572838   \n",
            "min      -154.183580    -181.953370     -81.794290    -188.214000   \n",
            "25%        -8.487185     -20.667008     -18.441185     -10.780267   \n",
            "50%        -0.652015      -6.007530     -11.187815      -2.047015   \n",
            "75%         8.788543       7.741405      -2.387207       6.508737   \n",
            "max       335.771820     262.068870     166.236890     172.402680   \n",
            "\n",
            "           -12.23257        7.83089  ...        13.0162      -54.40548  \\\n",
            "count  515130.000000  515130.000000  ...  515130.000000  515130.000000   \n",
            "mean       -1.793166       3.727748  ...      15.756104     -73.458195   \n",
            "std         7.964288      10.583763  ...      32.100772     175.616882   \n",
            "min       -72.503850    -126.479040  ...    -437.722030   -4402.376440   \n",
            "25%        -6.469023      -2.294058  ...      -1.813087    -139.546760   \n",
            "50%        -1.736475       3.822310  ...       9.171595     -53.084890   \n",
            "75%         2.913968       9.962117  ...      26.277387      13.483058   \n",
            "max       126.741270     146.297950  ...     840.973380    4469.454870   \n",
            "\n",
            "            58.99367       15.37344        1.11144      -23.08793  \\\n",
            "count  515130.000000  515130.000000  515130.000000  515130.000000   \n",
            "mean       41.545971      37.934308       0.314492      17.675010   \n",
            "std       122.230651      95.053404      16.161929     114.421362   \n",
            "min     -1810.689190   -3098.350310    -341.789120   -3168.924570   \n",
            "25%       -20.987987      -4.669885      -6.782665     -31.578587   \n",
            "50%        28.791115      33.622375       0.819805      15.597335   \n",
            "75%        89.665733      77.785443       8.470098      67.794305   \n",
            "max      3210.701700    1734.079690     260.544900    3662.065650   \n",
            "\n",
            "            68.40795       -1.82223      -27.46348        2.26327  \n",
            "count  515130.000000  515130.000000  515130.000000  515130.000000  \n",
            "mean      -26.320739       4.459232      20.034704       1.329613  \n",
            "std       173.989838      13.346688     185.570017      22.089498  \n",
            "min     -4319.992320    -236.039260   -7458.378150    -381.424430  \n",
            "25%      -101.533997      -2.565195     -59.508480      -8.819565  \n",
            "50%       -21.214665       3.118175       7.764265       0.053340  \n",
            "75%        52.379945       9.968190      86.351715       9.681062  \n",
            "max      2833.608950     463.419500    7393.398440     677.899630  \n",
            "\n",
            "[8 rows x 91 columns]\n"
          ]
        }
      ],
      "source": [
        "# Display basic information\n",
        "print(\"Data shape:\", df.shape)\n",
        "df.info()\n",
        "print(\"\\nDescriptive statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tul0nYHJxnH",
        "outputId": "61b01d79-ade0-4ac8-cc10-e7f10a442d48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing outliers...\n",
            "Removed 335697 outliers\n"
          ]
        }
      ],
      "source": [
        "# Remove outliers using IQR method\n",
        "print(\"Removing outliers...\")\n",
        "Q1 = df.quantile(0.25)\n",
        "Q3 = df.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "mask = ~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)\n",
        "df_clean = df[mask]\n",
        "print(f\"Removed {df.shape[0] - df_clean.shape[0]} outliers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vgvaGnaJ2TA",
        "outputId": "0dbc6b48-7c37-4e63-931b-a27af5e5f40a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-b7e1627af301>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_clean['target_clf'] = (df_clean[REG_TARGET] > df_clean[REG_TARGET].median()).astype(int)\n"
          ]
        }
      ],
      "source": [
        "# Define target variables\n",
        "REG_TARGET = df_clean.columns[0]\n",
        "df_clean['target_clf'] = (df_clean[REG_TARGET] > df_clean[REG_TARGET].median()).astype(int)\n",
        "CLS_TARGET = 'target_clf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xueQJLlFJ7I9"
      },
      "outputs": [],
      "source": [
        "# Visualize target distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "sns.histplot(df_clean[REG_TARGET], kde=True, ax=axes[0])\n",
        "axes[0].set_title('Distribution of Regression Target')\n",
        "sns.countplot(x=df_clean[CLS_TARGET], ax=axes[1])\n",
        "axes[1].set_title('Distribution of Classification Target')\n",
        "plt.tight_layout()\n",
        "plt.savefig('target_distributions.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "071-l3N2KD1e"
      },
      "outputs": [],
      "source": [
        "# Create correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "corr_matrix = df_clean.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', linewidths=0.5)\n",
        "plt.title('Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTP1RDsLKKI6",
        "outputId": "663fdfaa-7ca8-4b02-8dbf-23a1702adb8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing feature engineering...\n"
          ]
        }
      ],
      "source": [
        "# Feature engineering\n",
        "print(\"Performing feature engineering...\")\n",
        "features = [c for c in df_clean.columns if c not in [REG_TARGET, CLS_TARGET]]\n",
        "numeric_feats = df_clean[features].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categorical_feats = df_clean[features].select_dtypes(include=['object', 'category']).columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_bzijB-3KOMM"
      },
      "outputs": [],
      "source": [
        "# Advanced preprocessing pipeline with feature selection\n",
        "def create_advanced_pipeline(numeric_features, categorical_features):\n",
        "    \"\"\"\n",
        "    Create an advanced preprocessing pipeline with:\n",
        "    - PowerTransformer for numeric features to handle skewed distributions\n",
        "    - StandardScaler for normalized numeric features\n",
        "    - OneHotEncoder for categorical features\n",
        "    - Feature selection using SelectKBest\n",
        "    \"\"\"\n",
        "    numeric_pipeline = Pipeline([\n",
        "        ('power_transform', PowerTransformer(method='yeo-johnson', standardize=False)),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_pipeline = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_pipeline, numeric_features),\n",
        "            ('cat', categorical_pipeline, categorical_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    return preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lq2swhTaKQWs"
      },
      "outputs": [],
      "source": [
        "# Create the advanced preprocessing pipeline\n",
        "preprocessor = create_advanced_pipeline(numeric_feats, categorical_feats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9AV1CwaKT3w",
        "outputId": "c1e6d72f-34a2-46cc-90ab-93707f0cac40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splitting data into train and test sets...\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test sets\n",
        "print(\"Splitting data into train and test sets...\")\n",
        "X, y_reg, y_clf = df_clean[features], df_clean[REG_TARGET], df_clean[CLS_TARGET]\n",
        "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "    X, y_clf, test_size=0.2, stratify=y_clf, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynFd9TKAKWXG",
        "outputId": "388f81b9-50f0-4471-ecc8-8f9d016acb36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing data...\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "print(\"Preprocessing data...\")\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "X_train_clf_processed = preprocessor.transform(X_train_clf)\n",
        "X_test_clf_processed = preprocessor.transform(X_test_clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9oE5aJM3Kbmf"
      },
      "outputs": [],
      "source": [
        "# Get feature names after preprocessing\n",
        "feature_names = []\n",
        "if len(numeric_feats) > 0:\n",
        "    feature_names.extend(numeric_feats)\n",
        "if len(categorical_feats) > 0:\n",
        "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
        "    cat_features = ohe.get_feature_names_out(categorical_feats)\n",
        "    feature_names.extend(cat_features.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqPsXa8yKd62",
        "outputId": "9160b6a4-b071-4edf-f2f7-acb3e7f9b995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing feature selection for regression...\n"
          ]
        }
      ],
      "source": [
        "# Feature selection for regression\n",
        "print(\"Performing feature selection for regression...\")\n",
        "k_best = min(30, X_train_processed.shape[1])  # Select top 30 features or less if fewer features\n",
        "feature_selector = SelectKBest(f_regression, k=k_best)\n",
        "X_train_reg_selected = feature_selector.fit_transform(X_train_processed, y_train_reg)\n",
        "X_test_reg_selected = feature_selector.transform(X_test_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-GN1tp7KgzG",
        "outputId": "b79c19e7-ffda-48dd-97ca-7312bcb937c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing feature selection for classification...\n"
          ]
        }
      ],
      "source": [
        "# Feature selection for classification\n",
        "print(\"Performing feature selection for classification...\")\n",
        "feature_selector_clf = SelectKBest(mutual_info_regression, k=k_best)\n",
        "X_train_clf_selected = feature_selector_clf.fit_transform(X_train_clf_processed, y_train_clf)\n",
        "X_test_clf_selected = feature_selector_clf.transform(X_test_clf_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aVdWeDHqKipa"
      },
      "outputs": [],
      "source": [
        "# Advanced Regression Model\n",
        "def create_advanced_reg_model(input_dim):\n",
        "    \"\"\"Create an improved regression model with residual connections and regularization\"\"\"\n",
        "    inputs = layers.Input(shape=(input_dim,))\n",
        "\n",
        "    # First block\n",
        "    x = layers.Dense(256, kernel_regularizer=regularizers.l2(0.001))(inputs)\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "\n",
        "    # Residual block 1\n",
        "    res = x\n",
        "    x = layers.Dense(256, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.add([x, res])  # Residual connection\n",
        "\n",
        "    # Second block\n",
        "    x = layers.Dense(128, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "\n",
        "    # Residual block 2\n",
        "    res = layers.Dense(128)(x)  # Match dimensions\n",
        "    x = layers.Dense(128, kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.add([x, res])  # Residual connection\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(1, activation='linear')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name=\"advanced_regressor\")\n",
        "\n",
        "    # FIX: Use tf.keras.losses.Huber() instead of 'huber_loss' string\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "        loss=tf.keras.losses.Huber(),  # Huber loss is more robust to outliers\n",
        "        metrics=[\n",
        "            tf.keras.metrics.RootMeanSquaredError(),\n",
        "            tf.keras.metrics.MeanAbsoluteError()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZgb50iaKoc5",
        "outputId": "d02f1803-615c-4729-f697-0eb066d3c5bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performing cross-validation for regression model...\n",
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Cross-validation for regression model\n",
        "print(\"Performing cross-validation for regression model...\")\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "reg_cv_scores = []\n",
        "\n",
        "for train_idx, val_idx in kfold.split(X_train_reg_selected):\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train_reg_selected[train_idx], X_train_reg_selected[val_idx]\n",
        "    y_train_fold, y_val_fold = y_train_reg.iloc[train_idx], y_train_reg.iloc[val_idx]\n",
        "\n",
        "    # Create and train model\n",
        "    reg_model = create_advanced_reg_model(X_train_reg_selected.shape[1])\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=15, restore_best_weights=True)\n",
        "    reg_model.fit(\n",
        "        X_train_fold, y_train_fold,\n",
        "        validation_data=(X_val_fold, y_val_fold),\n",
        "        epochs=100, batch_size=32,\n",
        "        callbacks=[early_stop],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = reg_model.predict(X_val_fold).flatten()\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_fold, y_pred))\n",
        "    r2 = r2_score(y_val_fold, y_pred)\n",
        "    reg_cv_scores.append((rmse, r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJA5RgU1k2wF",
        "outputId": "d0401fd3-64f1-4dc8-8364-f0f9c3999341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Regression Cross-Validation Results:\n",
            "Mean RMSE: 6.6476 (±0.0498)\n",
            "Mean R²: 0.3313 (±0.0110)\n"
          ]
        }
      ],
      "source": [
        "# Print cross-validation results\n",
        "print(\"\\nRegression Cross-Validation Results:\")\n",
        "rmse_scores = [score[0] for score in reg_cv_scores]\n",
        "r2_scores = [score[1] for score in reg_cv_scores]\n",
        "print(f\"Mean RMSE: {np.mean(rmse_scores):.4f} (±{np.std(rmse_scores):.4f})\")\n",
        "print(f\"Mean R²: {np.mean(r2_scores):.4f} (±{np.std(r2_scores):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mK0tbOXk7K6",
        "outputId": "613c6845-f00f-4372-89fb-845c4bc28267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training final regression model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m3582/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 510.2239 - mean_absolute_error: 509.9566 - root_mean_squared_error: 832.1047"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 9ms/step - loss: 509.4441 - mean_absolute_error: 509.1766 - root_mean_squared_error: 831.3063 - val_loss: 7.5780 - val_mean_absolute_error: 7.1428 - val_root_mean_squared_error: 8.6081 - learning_rate: 0.0010\n",
            "Epoch 2/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - loss: 23.5234 - mean_absolute_error: 23.0640 - root_mean_squared_error: 29.1575 - val_loss: 8.1499 - val_mean_absolute_error: 7.5785 - val_root_mean_squared_error: 10.0753 - learning_rate: 0.0010\n",
            "Epoch 3/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - loss: 19.8516 - mean_absolute_error: 19.2638 - root_mean_squared_error: 24.2295 - val_loss: 7.6566 - val_mean_absolute_error: 7.0020 - val_root_mean_squared_error: 9.2125 - learning_rate: 0.0010\n",
            "Epoch 4/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step - loss: 18.0270 - mean_absolute_error: 17.3815 - root_mean_squared_error: 21.8952 - val_loss: 9.0959 - val_mean_absolute_error: 8.4741 - val_root_mean_squared_error: 10.7647 - learning_rate: 0.0010\n",
            "Epoch 5/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 17.2617 - mean_absolute_error: 16.6728 - root_mean_squared_error: 20.9365 - val_loss: 7.3092 - val_mean_absolute_error: 6.7978 - val_root_mean_squared_error: 9.0731 - learning_rate: 0.0010\n",
            "Epoch 6/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 17.0811 - mean_absolute_error: 16.6141 - root_mean_squared_error: 20.7839 - val_loss: 13.4226 - val_mean_absolute_error: 13.0624 - val_root_mean_squared_error: 14.7186 - learning_rate: 0.0010\n",
            "Epoch 7/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 9ms/step - loss: 16.4199 - mean_absolute_error: 16.0802 - root_mean_squared_error: 20.2226 - val_loss: 14.7038 - val_mean_absolute_error: 14.4351 - val_root_mean_squared_error: 15.5658 - learning_rate: 0.0010\n",
            "Epoch 8/25\n",
            "\u001b[1m3587/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16.6629 - mean_absolute_error: 16.4067 - root_mean_squared_error: 20.5792"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 10ms/step - loss: 16.6630 - mean_absolute_error: 16.4068 - root_mean_squared_error: 20.5793 - val_loss: 7.2349 - val_mean_absolute_error: 7.0194 - val_root_mean_squared_error: 8.2254 - learning_rate: 0.0010\n",
            "Epoch 9/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - loss: 16.3839 - mean_absolute_error: 16.1783 - root_mean_squared_error: 20.3417 - val_loss: 8.2883 - val_mean_absolute_error: 8.0967 - val_root_mean_squared_error: 10.1785 - learning_rate: 0.0010\n",
            "Epoch 10/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 9ms/step - loss: 17.2029 - mean_absolute_error: 17.0165 - root_mean_squared_error: 21.3129 - val_loss: 11.8172 - val_mean_absolute_error: 11.6445 - val_root_mean_squared_error: 13.2880 - learning_rate: 0.0010\n",
            "Epoch 11/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 16.3204 - mean_absolute_error: 16.1539 - root_mean_squared_error: 20.1890"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9ms/step - loss: 16.3205 - mean_absolute_error: 16.1540 - root_mean_squared_error: 20.1891 - val_loss: 6.6693 - val_mean_absolute_error: 6.5128 - val_root_mean_squared_error: 7.7490 - learning_rate: 0.0010\n",
            "Epoch 12/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 16.2586 - mean_absolute_error: 16.1088 - root_mean_squared_error: 20.3301 - val_loss: 7.6220 - val_mean_absolute_error: 7.4648 - val_root_mean_squared_error: 8.8139 - learning_rate: 0.0010\n",
            "Epoch 13/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - loss: 16.7622 - mean_absolute_error: 16.6080 - root_mean_squared_error: 20.9200 - val_loss: 11.8119 - val_mean_absolute_error: 11.6490 - val_root_mean_squared_error: 12.7474 - learning_rate: 0.0010\n",
            "Epoch 14/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 8ms/step - loss: 16.7672 - mean_absolute_error: 16.5981 - root_mean_squared_error: 20.8667 - val_loss: 8.7320 - val_mean_absolute_error: 8.5620 - val_root_mean_squared_error: 10.7856 - learning_rate: 0.0010\n",
            "Epoch 15/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 16.5587 - mean_absolute_error: 16.3943 - root_mean_squared_error: 20.6311 - val_loss: 8.9030 - val_mean_absolute_error: 8.7417 - val_root_mean_squared_error: 10.7003 - learning_rate: 0.0010\n",
            "Epoch 16/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 16.6662 - mean_absolute_error: 16.5157 - root_mean_squared_error: 20.6771 - val_loss: 7.1312 - val_mean_absolute_error: 6.9836 - val_root_mean_squared_error: 9.5719 - learning_rate: 0.0010\n",
            "Epoch 17/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - loss: 16.4792 - mean_absolute_error: 16.3465 - root_mean_squared_error: 20.5399 - val_loss: 9.4889 - val_mean_absolute_error: 9.3558 - val_root_mean_squared_error: 11.5197 - learning_rate: 0.0010\n",
            "Epoch 18/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 8ms/step - loss: 16.3776 - mean_absolute_error: 16.2428 - root_mean_squared_error: 20.4649 - val_loss: 19.1141 - val_mean_absolute_error: 18.9774 - val_root_mean_squared_error: 19.9941 - learning_rate: 0.0010\n",
            "Epoch 19/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 9ms/step - loss: 16.0999 - mean_absolute_error: 15.9637 - root_mean_squared_error: 20.0845 - val_loss: 7.8748 - val_mean_absolute_error: 7.7392 - val_root_mean_squared_error: 8.8725 - learning_rate: 0.0010\n",
            "Epoch 20/25\n",
            "\u001b[1m3585/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15.9049 - mean_absolute_error: 15.7684 - root_mean_squared_error: 19.8296"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 15.9053 - mean_absolute_error: 15.7688 - root_mean_squared_error: 19.8301 - val_loss: 5.3180 - val_mean_absolute_error: 5.1716 - val_root_mean_squared_error: 6.8608 - learning_rate: 0.0010\n",
            "Epoch 21/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 8ms/step - loss: 16.2806 - mean_absolute_error: 16.1501 - root_mean_squared_error: 20.2075 - val_loss: 5.5200 - val_mean_absolute_error: 5.3712 - val_root_mean_squared_error: 7.4926 - learning_rate: 0.0010\n",
            "Epoch 22/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 16.3411 - mean_absolute_error: 16.2194 - root_mean_squared_error: 20.3686 - val_loss: 14.1631 - val_mean_absolute_error: 14.0622 - val_root_mean_squared_error: 15.1889 - learning_rate: 0.0010\n",
            "Epoch 23/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 16.1978 - mean_absolute_error: 16.0938 - root_mean_squared_error: 20.2096 - val_loss: 6.1082 - val_mean_absolute_error: 5.9988 - val_root_mean_squared_error: 7.2527 - learning_rate: 0.0010\n",
            "Epoch 24/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 9ms/step - loss: 16.0891 - mean_absolute_error: 15.9843 - root_mean_squared_error: 20.0734 - val_loss: 14.4185 - val_mean_absolute_error: 14.3213 - val_root_mean_squared_error: 15.7869 - learning_rate: 0.0010\n",
            "Epoch 25/25\n",
            "\u001b[1m3589/3589\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 8ms/step - loss: 16.1049 - mean_absolute_error: 16.0004 - root_mean_squared_error: 20.2115 - val_loss: 12.5524 - val_mean_absolute_error: 12.4420 - val_root_mean_squared_error: 13.5701 - learning_rate: 0.0010\n"
          ]
        }
      ],
      "source": [
        "# Train final regression model\n",
        "print(\"\\nTraining final regression model...\")\n",
        "final_reg_model = create_advanced_reg_model(X_train_reg_selected.shape[1])\n",
        "reg_callbacks = [\n",
        "    callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=20, restore_best_weights=True),\n",
        "    callbacks.ModelCheckpoint('advanced_regressor.h5', save_best_only=True, monitor='val_root_mean_squared_error'),\n",
        "    callbacks.ReduceLROnPlateau(monitor='val_root_mean_squared_error', factor=0.5, patience=10, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "history_reg = final_reg_model.fit(\n",
        "    X_train_reg_selected, y_train_reg,\n",
        "    validation_split=0.2,\n",
        "    epochs=25, batch_size=32,\n",
        "    callbacks=reg_callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lutsA4L5oqh1"
      },
      "outputs": [],
      "source": [
        "# Plot training history for regression\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_reg.history['root_mean_squared_error'])\n",
        "plt.plot(history_reg.history['val_root_mean_squared_error'])\n",
        "plt.title('Model RMSE')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_reg.history['mean_absolute_error'])\n",
        "plt.plot(history_reg.history['val_mean_absolute_error'])\n",
        "plt.title('Model MAE')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('regression_training_history.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f86BrtOTo5-i",
        "outputId": "cb08d823-70a0-4fd5-c4fc-3e4b0ee9c9d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating regression model on test set...\n",
            "\u001b[1m1122/1122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
            "MSE: 46.7271, RMSE: 6.8357, R²: 0.2973\n"
          ]
        }
      ],
      "source": [
        "# Evaluate regression model on test set\n",
        "print(\"\\nEvaluating regression model on test set...\")\n",
        "y_pred_reg = final_reg_model.predict(X_test_reg_selected).flatten()\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "print(f'MSE: {mse:.4f}, RMSE: {rmse:.4f}, R²: {r2:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZWR2qJT1o9Mc"
      },
      "outputs": [],
      "source": [
        "# Plot actual vs predicted for regression\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test_reg, y_pred_reg, alpha=0.5)\n",
        "plt.plot(\n",
        "    [y_test_reg.min(), y_test_reg.max()],\n",
        "    [y_test_reg.min(), y_test_reg.max()],\n",
        "    'r--'\n",
        ")\n",
        "plt.title('Actual vs Predicted (Regression)')\n",
        "plt.xlabel('Actual')\n",
        "plt.ylabel('Predicted')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('regression_actual_vs_predicted.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoMY-eR8o_c_",
        "outputId": "642c3ab1-a308-4b69-bd30-6d2c2eab02ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Preparing data for classification model...\n"
          ]
        }
      ],
      "source": [
        "# Advanced Classification Model with Hyperparameter Tuning\n",
        "print(\"\\nPreparing data for classification model...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dKXWq1opGQI",
        "outputId": "27c9e977-97bc-44c7-ebf7-f9b497b8cd79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying SMOTETomek for handling class imbalance...\n"
          ]
        }
      ],
      "source": [
        "# Handle class imbalance with SMOTETomek (combines over-sampling and under-sampling)\n",
        "print(\"Applying SMOTETomek for handling class imbalance...\")\n",
        "smote_tomek = SMOTETomek(random_state=42)\n",
        "X_train_clf_resampled, y_train_clf_resampled = smote_tomek.fit_resample(X_train_clf_selected, y_train_clf)\n",
        "\n",
        "def build_advanced_clf_model(hp):\n",
        "    \"\"\"Create an improved classification model with hyperparameter tuning\"\"\"\n",
        "    inputs = layers.Input(shape=(X_train_clf_selected.shape[1],))\n",
        "\n",
        "    # First layer\n",
        "    x = layers.Dense(\n",
        "        hp.Int('units_1', min_value=64, max_value=256, step=32),\n",
        "        kernel_regularizer=regularizers.l2(hp.Float('l2_1', 1e-4, 1e-2, sampling='log'))\n",
        "    )(inputs)\n",
        "    x = layers.LeakyReLU(alpha=hp.Float('alpha_1', 0.05, 0.3, step=0.05))(x)\n",
        "    if hp.Boolean('batch_norm_1'):\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(hp.Float('dropout_1', 0.2, 0.5, step=0.1))(x)\n",
        "\n",
        "    # Hidden layers\n",
        "    for i in range(2, hp.Int('num_layers', 2, 5) + 1):\n",
        "        x = layers.Dense(\n",
        "            hp.Int(f'units_{i}', min_value=32, max_value=128, step=32),\n",
        "            kernel_regularizer=regularizers.l2(hp.Float(f'l2_{i}', 1e-4, 1e-2, sampling='log'))\n",
        "        )(x)\n",
        "        x = layers.LeakyReLU(alpha=hp.Float(f'alpha_{i}', 0.05, 0.3, step=0.05))(x)\n",
        "        if hp.Boolean(f'batch_norm_{i}'):\n",
        "            x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(hp.Float(f'dropout_{i}', 0.1, 0.4, step=0.1))(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(\n",
        "            learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
        "        ),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy',\n",
        "                 tf.keras.metrics.AUC(name='auc'),\n",
        "                 tf.keras.metrics.Precision(name='precision'),\n",
        "                 tf.keras.metrics.Recall(name='recall')]\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFNldrQEqGR2",
        "outputId": "6dec911b-70d8-4e80-f7fd-70fc56d13f01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting up hyperparameter tuning for classification model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "print(\"Setting up hyperparameter tuning for classification model...\")\n",
        "tuner = kt.BayesianOptimization(\n",
        "    build_advanced_clf_model,\n",
        "    objective=kt.Objective('val_auc', direction='max'),\n",
        "    max_trials=5,\n",
        "    directory='kt_dir_advanced',\n",
        "    project_name='advanced_mlp_clf',\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "s5do7sR_qL4P"
      },
      "outputs": [],
      "source": [
        "# Callbacks for tuner\n",
        "stop_early = callbacks.EarlyStopping(monitor='val_auc', patience=10, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfZKN8HbqPtq",
        "outputId": "dc7abd21-afe1-42ff-96a3-6f2a99a2bbe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 00m 57s]\n",
            "val_auc: 0.8023454546928406\n",
            "\n",
            "Best val_auc So Far: 0.8067225813865662\n",
            "Total elapsed time: 00h 05m 44s\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting hyperparameter search...\")\n",
        "tuner.search(\n",
        "    X_train_clf_resampled, y_train_clf_resampled,\n",
        "    epochs=3,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[stop_early],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3JxENDfwdFP",
        "outputId": "8e833d2d-d976-4861-eda3-6c7a18872524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best hyperparameters: {'units_1': 256, 'l2_1': 0.0003602330794921003, 'alpha_1': 0.25, 'batch_norm_1': True, 'dropout_1': 0.2, 'num_layers': 3, 'units_2': 96, 'l2_2': 0.0019990629503147854, 'alpha_2': 0.2, 'batch_norm_2': True, 'dropout_2': 0.2, 'learning_rate': 0.0033439519045640126, 'units_3': 32, 'l2_3': 0.0001, 'alpha_3': 0.05, 'batch_norm_3': False, 'dropout_3': 0.1}\n"
          ]
        }
      ],
      "source": [
        "# Get best hyperparameters\n",
        "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"\\nBest hyperparameters:\", best_hp.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVSAhayOwgvU",
        "outputId": "df076356-9dfb-4779-aeb8-fe341f8b9f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training final classification model with best hyperparameters...\n",
            "Epoch 1/5\n",
            "\u001b[1m3554/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7092 - auc: 0.7722 - loss: 0.6849 - precision: 0.6842 - recall: 0.7208"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3555/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.7092 - auc: 0.7722 - loss: 0.6848 - precision: 0.6842 - recall: 0.7208 - val_accuracy: 0.7260 - val_auc: 0.8023 - val_loss: 0.5796 - val_precision: 0.8010 - val_recall: 0.7182 - learning_rate: 0.0033\n",
            "Epoch 2/5\n",
            "\u001b[1m3549/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7239 - auc: 0.7898 - loss: 0.5800 - precision: 0.6944 - recall: 0.7482"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m3555/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 5ms/step - accuracy: 0.7239 - auc: 0.7898 - loss: 0.5800 - precision: 0.6944 - recall: 0.7482 - val_accuracy: 0.7288 - val_auc: 0.8013 - val_loss: 0.5693 - val_precision: 0.7947 - val_recall: 0.7340 - learning_rate: 0.0033\n",
            "Epoch 3/5\n",
            "\u001b[1m3555/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 6ms/step - accuracy: 0.7257 - auc: 0.7921 - loss: 0.5700 - precision: 0.6980 - recall: 0.7452 - val_accuracy: 0.7318 - val_auc: 0.8042 - val_loss: 0.5668 - val_precision: 0.7965 - val_recall: 0.7380 - learning_rate: 0.0033\n",
            "Epoch 4/5\n",
            "\u001b[1m3555/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - accuracy: 0.7252 - auc: 0.7929 - loss: 0.5661 - precision: 0.6976 - recall: 0.7442 - val_accuracy: 0.7242 - val_auc: 0.8035 - val_loss: 0.5742 - val_precision: 0.8063 - val_recall: 0.7064 - learning_rate: 0.0033\n",
            "Epoch 5/5\n",
            "\u001b[1m3555/3555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 5ms/step - accuracy: 0.7271 - auc: 0.7941 - loss: 0.5638 - precision: 0.6997 - recall: 0.7458 - val_accuracy: 0.7294 - val_auc: 0.8070 - val_loss: 0.5622 - val_precision: 0.8032 - val_recall: 0.7225 - learning_rate: 0.0033\n"
          ]
        }
      ],
      "source": [
        "# Build and train best model\n",
        "print(\"\\nTraining final classification model with best hyperparameters...\")\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "clf_callbacks = [\n",
        "    callbacks.EarlyStopping(monitor='val_auc', patience=15, restore_best_weights=True),\n",
        "    callbacks.ModelCheckpoint('advanced_classifier.h5', save_best_only=True, monitor='val_auc'),\n",
        "    callbacks.ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=7, min_lr=1e-6)\n",
        "]\n",
        "\n",
        "history_clf = best_model.fit(\n",
        "    X_train_clf_resampled, y_train_clf_resampled,\n",
        "    validation_split=0.2,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    callbacks=clf_callbacks,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "vfmCeFaFw7Xo"
      },
      "outputs": [],
      "source": [
        "# Plot training history for classification\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history_clf.history['accuracy'])\n",
        "plt.plot(history_clf.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history_clf.history['auc'])\n",
        "plt.plot(history_clf.history['val_auc'])\n",
        "plt.title('Model AUC')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='lower right')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history_clf.history['loss'])\n",
        "plt.plot(history_clf.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.tight_layout()\n",
        "plt.savefig('classification_training_history.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkoutNkBxHVL",
        "outputId": "efebd010-22be-4230-9b24-2e1aabd93f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating classification model on test set...\n",
            "\u001b[1m1122/1122\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Evaluate classification model\n",
        "print(\"\\nEvaluating classification model on test set...\")\n",
        "y_proba = best_model.predict(X_test_clf_selected)\n",
        "y_pred = (y_proba > 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgVSFvl6xO5I",
        "outputId": "aae0b37c-dabf-44b1-a4f1-eaea06aea458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7186, Precision: 0.7042, Recall: 0.7083, F1: 0.7063, AUC: 0.7910\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.73      0.73     18745\n",
            "           1       0.70      0.71      0.71     17142\n",
            "\n",
            "    accuracy                           0.72     35887\n",
            "   macro avg       0.72      0.72      0.72     35887\n",
            "weighted avg       0.72      0.72      0.72     35887\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate metrics\n",
        "acc = accuracy_score(y_test_clf, y_pred)\n",
        "prec = precision_score(y_test_clf, y_pred)\n",
        "rec = recall_score(y_test_clf, y_pred)\n",
        "f1 = f1_score(y_test_clf, y_pred)\n",
        "auc = roc_auc_score(y_test_clf, y_proba)\n",
        "\n",
        "print(f'Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {auc:.4f}')\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test_clf, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "OpxA8oAIxQZP"
      },
      "outputs": [],
      "source": [
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test_clf, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "fpr, tpr, _ = roc_curve(y_test_clf, y_proba)\n",
        "plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
        "plt.plot([0, 1], [0, 1], 'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('roc_curve.png')\n",
        "plt.close()\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "precision, recall, _ = precision_recall_curve(y_test_clf, y_proba)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('precision_recall_curve.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0YpD-K_xUQc",
        "outputId": "05f0a837-5f01-4ce3-ed34-bf9eb21e1282"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calculating feature importance using permutation importance...\n",
            "Error calculating feature importance: The 'estimator' parameter of permutation_importance must be an object implementing 'fit'. Got <function <lambda> at 0x7f47e4477380> instead.\n",
            "Falling back to simpler feature importance visualization...\n",
            "\n",
            "Model training and evaluation complete!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nCalculating feature importance using permutation importance...\")\n",
        "try:\n",
        "    from sklearn.inspection import permutation_importance\n",
        "\n",
        "    # For regression model\n",
        "    result_reg = permutation_importance(\n",
        "        lambda X: final_reg_model.predict(X),\n",
        "        X_test_reg_selected,\n",
        "        y_test_reg,\n",
        "        n_repeats=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Plot feature importance for regression\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sorted_idx = result_reg.importances_mean.argsort()\n",
        "    plt.barh(range(len(sorted_idx)), result_reg.importances_mean[sorted_idx])\n",
        "    plt.yticks(range(len(sorted_idx)), [f\"Feature {i}\" for i in sorted_idx])\n",
        "    plt.xlabel(\"Permutation Importance\")\n",
        "    plt.title(\"Feature Importance (Regression)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('importance_regression.png')\n",
        "    plt.close()\n",
        "\n",
        "    # For classification model\n",
        "    result_clf = permutation_importance(\n",
        "        lambda X: best_model.predict(X),\n",
        "        X_test_clf_selected,\n",
        "        y_test_clf,\n",
        "        n_repeats=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    # Plot feature importance for classification\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sorted_idx = result_clf.importances_mean.argsort()\n",
        "    plt.barh(range(len(sorted_idx)), result_clf.importances_mean[sorted_idx])\n",
        "    plt.yticks(range(len(sorted_idx)), [f\"Feature {i}\" for i in sorted_idx])\n",
        "    plt.xlabel(\"Permutation Importance\")\n",
        "    plt.title(\"Feature Importance (Classification)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('importance_classification.png')\n",
        "    plt.close()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error calculating feature importance: {e}\")\n",
        "    # Fallback to simple coefficient-based feature importance\n",
        "    print(\"Falling back to simpler feature importance visualization...\")\n",
        "\n",
        "    # For regression - Extract weights from the first dense layer\n",
        "    reg_weights = np.abs(final_reg_model.layers[1].get_weights()[0]).mean(axis=1)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.bar(range(len(reg_weights)), reg_weights)\n",
        "    plt.title('Simple Feature Importance (Regression)')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Absolute Weight')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('simple_importance_regression.png')\n",
        "    plt.close()\n",
        "\n",
        "    # For classification - Extract weights from the first dense layer\n",
        "    clf_weights = np.abs(best_model.layers[1].get_weights()[0]).mean(axis=1)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.bar(range(len(clf_weights)), clf_weights)\n",
        "    plt.title('Simple Feature Importance (Classification)')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Absolute Weight')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('simple_importance_classification.png')\n",
        "    plt.close()\n",
        "\n",
        "print(\"\\nModel training and evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![My image](./target_distributions.png)\n",
        "![My image](./correlation_matrix.png)\n",
        "![My image](./regression_training_history.png)\n",
        "![My image](./regression_actual_vs_predicted.png)\n",
        "![My image](./classification_training_history.png)\n",
        "![My image](./confusion_matrix.png)\n",
        "![My image](./roc_curve.png)\n",
        "![My image](./precision_recall_curve.png)\n",
        "![My image](./simple_importance_regression.png)\n",
        "![My image](./simple_importance_classification.png)\n",
        "\n",
        "## Matriks Evaluasi Regresi (MSE: 46.7271, RMSE: 6.8357, R²: 0.2973)\n",
        "Model menunjukkan performa yang kurang baik, dengan MSE dan RMSE yang cukup tinggi serta R² hanya 0.2973, yang berarti model hanya menjelaskan sekitar 30% variasi data. Perbaikan dapat dilakukan dengan mencoba algoritma lain atau menambahkan fitur yang lebih relevan.\n",
        "\n",
        "## Matriks Evaluasi Klasifikasi\n",
        "Model menunjukkan performa yang cukup baik, dengan **accuracy** 71.86%, **precision** 70.42%, **recall** 70.83%, dan **F1-score** 70.63%. **AUC** 0.7910 menunjukkan model mampu membedakan kelas dengan baik. Secara keseluruhan, kinerja model solid, meski masih bisa ditingkatkan."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
